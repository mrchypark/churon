---
title: "Introduction to churon: ONNX Runtime for R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to churon: ONNX Runtime for R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE  # Set to FALSE since ONNX Runtime may not be available in all environments
)
```

```{r setup}
library(churon)
```

## Introduction

The **churon** package provides R bindings for ONNX Runtime, enabling high-performance machine learning inference directly in R. This vignette introduces the main features and demonstrates how to use the package effectively.

## What is ONNX?

ONNX (Open Neural Network Exchange) is an open standard for representing machine learning models. It allows models trained in different frameworks (PyTorch, TensorFlow, scikit-learn, etc.) to be converted to a common format and run efficiently using ONNX Runtime.

## Key Features of churon

- **High Performance**: Built with Rust for memory safety and speed
- **Multiple Execution Providers**: Support for CPU, CUDA, TensorRT, and more
- **Korean Text Processing**: Specialized support for Korean language models
- **Comprehensive Error Handling**: Clear error messages and validation
- **Easy-to-Use API**: Simple and intuitive R interface

## Installation and Setup

### Prerequisites

Before using churon, you need to install ONNX Runtime on your system. Please refer to the [ONNX Runtime installation guide](https://onnxruntime.ai/docs/install/) for your platform.

### Checking ONNX Runtime Availability

```{r check-runtime}
# Check if ONNX Runtime is available
if (check_onnx_runtime()) {
  cat("ONNX Runtime is available and working!\n")
} else {
  cat("ONNX Runtime is not available. Please install it first.\n")
}
```

## Basic Usage

### Loading Example Models

The churon package includes example models for demonstration:

```{r example-models}
# List available example models
models <- onnx_example_models()
print(models)

# The package includes Korean spacing models
if (length(models) > 0) {
  cat("Available models:\n")
  for (i in seq_along(models)) {
    cat(sprintf("  %s: %s\n", names(models)[i], models[i]))
  }
}
```

### Creating a Session

```{r create-session}
# Create a session with an example model
if (length(models) > 0) {
  # Method 1: Using model path directly
  session <- onnx_session(models[1])
  
  # Method 2: Using example session helper
  session <- onnx_example_session("kospacing")
  
  # Method 3: Safe session creation
  session <- safe_onnx_session(models[1])
  
  if (!is.null(session)) {
    cat("Session created successfully!\n")
  }
}
```

### Exploring Model Information

```{r model-info}
if (!is.null(session)) {
  # Get input information
  input_info <- onnx_input_info(session)
  cat("Model inputs:\n")
  print(input_info)
  
  # Get output information
  output_info <- onnx_output_info(session)
  cat("Model outputs:\n")
  print(output_info)
  
  # Get execution providers
  providers <- onnx_providers(session)
  cat("Execution providers:", paste(providers, collapse = ", "), "\n")
  
  # Get model path
  model_path <- onnx_model_path(session)
  cat("Model path:", model_path, "\n")
}
```

## Running Inference

### Basic Inference

```{r basic-inference}
if (!is.null(session)) {
  # Prepare input data (this is model-specific)
  # For demonstration, we'll use dummy data
  # In practice, you would prepare data according to your model's requirements
  
  # Example: if the model expects a matrix input named "input"
  input_data <- list(
    input = matrix(rnorm(100), nrow = 10, ncol = 10)
  )
  
  # Run inference
  tryCatch({
    result <- onnx_run(session, input_data)
    cat("Inference completed successfully!\n")
    print(str(result))
  }, error = function(e) {
    cat("Inference failed:", e$message, "\n")
    cat("This is expected with dummy data - use real model inputs\n")
  })
}
```

### Safe Inference

```{r safe-inference}
if (!is.null(session)) {
  # Safe inference with error handling
  result <- safe_onnx_run(session, input_data, silent = TRUE)
  
  if (!is.null(result)) {
    cat("Safe inference completed successfully!\n")
  } else {
    cat("Safe inference returned NULL (expected with dummy data)\n")
  }
}
```

## Error Handling

### Common Error Scenarios

```{r error-handling}
# Example 1: Invalid model path
invalid_session <- safe_onnx_session("nonexistent_model.onnx", silent = TRUE)
cat("Invalid session result:", is.null(invalid_session), "\n")

# Example 2: Invalid inputs
if (!is.null(session)) {
  # Missing required inputs
  invalid_result <- safe_onnx_run(session, list(), silent = TRUE)
  cat("Invalid inference result:", is.null(invalid_result), "\n")
  
  # Wrong input names
  wrong_inputs <- list(wrong_name = matrix(1:10, nrow = 2))
  wrong_result <- safe_onnx_run(session, wrong_inputs, silent = TRUE)
  cat("Wrong input names result:", is.null(wrong_result), "\n")
}
```

### Comprehensive Error Handling

```{r comprehensive-error-handling}
# Complete workflow with error handling
complete_workflow <- function(model_path, input_data) {
  tryCatch({
    # Step 1: Create session
    cat("Creating session...\n")
    session <- onnx_session(model_path)
    
    # Step 2: Validate inputs against model requirements
    input_info <- onnx_input_info(session)
    cat("Model expects", length(input_info), "inputs\n")
    
    # Step 3: Run inference
    cat("Running inference...\n")
    result <- onnx_run(session, input_data)
    
    cat("Workflow completed successfully!\n")
    return(result)
    
  }, error = function(e) {
    if (grepl("ONNX Runtime library not found", e$message)) {
      cat("Error: ONNX Runtime not installed\n")
      cat("Solution: Install ONNX Runtime for your platform\n")
    } else if (grepl("Model file not found", e$message)) {
      cat("Error: Model file not found\n")
      cat("Solution: Check the file path\n")
    } else if (grepl("Required input.*not provided", e$message)) {
      cat("Error: Missing required inputs\n")
      cat("Solution: Check model requirements with onnx_input_info()\n")
    } else {
      cat("Error:", e$message, "\n")
    }
    return(NULL)
  })
}

# Example usage (will fail gracefully with dummy data)
if (length(models) > 0) {
  result <- complete_workflow(models[1], list(dummy = matrix(1:4, nrow = 2)))
}
```

## Execution Providers

### Specifying Execution Providers

```{r execution-providers}
# List of available providers
valid_providers <- c("cuda", "tensorrt", "directml", "onednn", "coreml", "cpu")

# Create session with specific providers
if (length(models) > 0) {
  # Try CUDA first, fallback to CPU
  gpu_session <- safe_onnx_session(models[1], providers = c("cuda", "cpu"))
  
  # CPU only
  cpu_session <- safe_onnx_session(models[1], providers = "cpu")
  
  if (!is.null(gpu_session)) {
    cat("GPU session providers:", paste(onnx_providers(gpu_session), collapse = ", "), "\n")
  }
  
  if (!is.null(cpu_session)) {
    cat("CPU session providers:", paste(onnx_providers(cpu_session), collapse = ", "), "\n")
  }
}
```

## Working with Korean Text Models

### Korean Spacing Models

The package includes Korean spacing (kospacing) models for text processing:

```{r korean-models}
# Load Korean spacing model
korean_session <- safe_onnx_session(find_model_path("kospacing"))

if (!is.null(korean_session)) {
  cat("Korean spacing model loaded successfully!\n")
  
  # Get model information
  input_info <- onnx_input_info(korean_session)
  cat("Korean model input requirements:\n")
  print(input_info)
  
  # Note: Actual usage would require proper text preprocessing
  # according to the model's requirements
}
```

## Best Practices

### 1. Always Check ONNX Runtime Availability

```{r best-practice-1}
if (!check_onnx_runtime()) {
  stop("ONNX Runtime is required but not available")
}
```

### 2. Use Safe Functions for Production Code

```{r best-practice-2}
# Instead of:
# session <- onnx_session(model_path)

# Use:
session <- safe_onnx_session(model_path)
if (is.null(session)) {
  stop("Failed to create session")
}
```

### 3. Validate Inputs Before Inference

```{r best-practice-3}
validate_inputs <- function(session, inputs) {
  input_info <- onnx_input_info(session)
  
  # Check if all required inputs are provided
  required_names <- sapply(input_info, function(x) x$get_name())
  provided_names <- names(inputs)
  
  missing <- setdiff(required_names, provided_names)
  if (length(missing) > 0) {
    stop("Missing required inputs: ", paste(missing, collapse = ", "))
  }
  
  # Additional validation can be added here
  return(TRUE)
}
```

### 4. Handle Errors Gracefully

```{r best-practice-4}
robust_inference <- function(model_path, inputs) {
  session <- safe_onnx_session(model_path)
  if (is.null(session)) return(NULL)
  
  if (!validate_inputs(session, inputs)) return(NULL)
  
  result <- safe_onnx_run(session, inputs)
  return(result)
}
```

## Conclusion

The churon package provides a robust and efficient way to run ONNX models in R. Key takeaways:

- Always check ONNX Runtime availability before use
- Use safe functions for production code
- Validate inputs against model requirements
- Handle errors gracefully with appropriate error messages
- Consider execution providers for optimal performance

For more information and examples, see the package documentation and GitHub repository.